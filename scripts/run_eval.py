import argparse
import math
import os
import yaml
import pandas as pd
import torch
import torch.distributed as dist
import wandb
from transformers import AutoImageProcessor, AutoModelForVideoClassification

from info_rates.analysis.evaluate import (
    evaluate_fixed_parallel,
    evaluate_fixed_parallel_counts,
    per_class_analysis_fast,
)

# Optional: plotting (requires matplotlib, seaborn)
try:
    from info_rates.viz.plots import plot_accuracy_curves, plot_heatmap
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False


def wilson_ci(correct: int, total: int, z: float = 1.96) -> tuple[float, float]:
    if total <= 0:
        return 0.0, 0.0
    p = correct / total
    denom = 1 + z**2 / total
    centre = p + z**2 / (2 * total)
    margin = z * math.sqrt((p * (1 - p) + z**2 / (4 * total)) / total)
    low = max(0.0, (centre - margin) / denom)
    high = min(1.0, (centre + margin) / denom)
    return low, high


def compute_pareto(df: pd.DataFrame) -> pd.DataFrame:
    """Extract Pareto-optimal (accuracy, latency) frontier."""
    df_sorted = df.sort_values("avg_time")
    frontier = []
    best_acc = -1.0
    for _, row in df_sorted.iterrows():
        acc = row["accuracy"]
        if acc >= best_acc:
            frontier.append(row)
            best_acc = acc
    return pd.DataFrame(frontier)


def main():
    parser = argparse.ArgumentParser(description="Evaluate temporal sampling effects on fixed-length clips.")
    parser.add_argument("--config", type=str, default="config.yaml", help="Path to config file")
    parser.add_argument("--model-path", type=str, help="Path to saved model directory")
    parser.add_argument("--manifest", type=str, help="CSV manifest with columns: video_path,label")
    parser.add_argument("--coverages", nargs="*", type=int)
    parser.add_argument("--strides", nargs="*", type=int)
    parser.add_argument("--sample-size", type=int)
    parser.add_argument("--batch-size", type=int)
    parser.add_argument("--workers", type=int)
    parser.add_argument("--out", type=str)
    parser.add_argument("--wandb-project", default="inforates-ucf101", help="WandB project name")
    parser.add_argument("--wandb-run-name", default=None, help="WandB run name")
    parser.add_argument("--no-wandb", action="store_true", help="Disable WandB logging")
    parser.add_argument("--ddp", action="store_true", help="Enable Distributed Data Parallel evaluation")
    parser.add_argument("--per-class", action="store_true", help="Compute per-class metrics as well")
    parser.add_argument("--per-class-out", type=str, help="Path to save per-class CSV")
    parser.add_argument("--per-class-sample-size", type=int, help="Limit per-class eval to N samples for speed")
    parser.add_argument("--jitter-coverage-pct", type=float, help="Randomly jitter coverage Â±pct during eval for robustness checks")
    args = parser.parse_args()

    # Load config and apply defaults
    if os.path.exists(args.config):
        with open(args.config, "r") as f:
            cfg = yaml.safe_load(f)
    else:
        cfg = {}

    model_path = args.model_path or cfg.get("save_path", "models/timesformer_ucf101_ddp")
    manifest = args.manifest or cfg.get("eval_manifest", "UCF101_data/manifests/ucf101_50f.csv")
    coverages = args.coverages if args.coverages else cfg.get("eval_coverages", [10, 25, 50, 75, 100])
    strides = args.strides if args.strides else cfg.get("eval_strides", [1, 2, 4, 8, 16])
    sample_size = args.sample_size if args.sample_size is not None else int(cfg.get("eval_sample_size", 200))
    batch_size = args.batch_size if args.batch_size is not None else int(cfg.get("eval_batch_size", 8))
    workers = args.workers if args.workers is not None else int(cfg.get("eval_workers", 8))
    out_path = args.out or cfg.get("eval_out", "UCF101_data/results/ucf101_50f_finetuned.csv")
    wandb_project = args.wandb_project or cfg.get("wandb_project", "inforates-ucf101")
    wandb_run_name = args.wandb_run_name or cfg.get("eval_wandb_run_name")
    ddp = args.ddp or bool(cfg.get("use_ddp", False))
    do_per_class = args.per_class or bool(cfg.get("eval_per_class", False))
    per_class_out = args.per_class_out or cfg.get("eval_per_class_out", "UCF101_data/results/ucf101_50f_per_class.csv")
    per_class_sample_size = (
        args.per_class_sample_size if args.per_class_sample_size is not None else int(cfg.get("eval_per_class_sample_size", -1))
    )  # -1 means full
    jitter_coverage_pct = args.jitter_coverage_pct if args.jitter_coverage_pct is not None else float(cfg.get("eval_jitter_coverage_pct", 0.0))

    # Setup DDP if requested
    rank = 0
    world_size = 1
    if ddp:
        backend = "nccl" if torch.cuda.is_available() else "gloo"
        dist.init_process_group(backend=backend)
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if torch.cuda.is_available():
            torch.cuda.set_device(local_rank)

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Initialize WandB (only rank 0 when DDP)
    if not args.no_wandb and (not ddp or rank == 0):
        wandb.init(
            project=wandb_project,
            name=wandb_run_name,
            config={
                "model_path": model_path,
                "coverages": coverages,
                "strides": strides,
                "sample_size": sample_size,
                "batch_size": batch_size,
                "workers": workers,
                "ddp": ddp,
                "world_size": world_size,
                "jitter_coverage_pct": jitter_coverage_pct,
            }
        )

    processor = AutoImageProcessor.from_pretrained(model_path)
    model = AutoModelForVideoClassification.from_pretrained(model_path).to(device).eval()

    df = pd.read_csv(manifest)

    if ddp and world_size > 1:
        # Ensure same subset across ranks; sample_size<=0 means full dataset
        if sample_size is not None and sample_size > 0 and sample_size < len(df):
            subset = df.sample(sample_size, random_state=42)
        else:
            subset = df
        # Shard rows across ranks
        my_subset = subset.iloc[rank::world_size]
        local_counts = evaluate_fixed_parallel_counts(
            df=my_subset,
            processor=processor,
            model=model,
            coverages=coverages,
            strides=strides,
            sample_size=len(my_subset),
            batch_size=batch_size,
            num_workers=workers,
            jitter_coverage_pct=jitter_coverage_pct,
        )

        # Aggregate counts across ranks
        # Build a stable ordering of (coverage, stride)
        combos = [(cov, st) for st in strides for cov in coverages]
        agg_rows = []
        for cov, st in combos:
            # Find local row
            row = next((r for r in local_counts if r["coverage"] == cov and r["stride"] == st), {
                "coverage": cov, "stride": st, "correct": 0, "total": 0, "total_time": 0.0
            })
            tensor = torch.tensor([row["correct"], row["total"], row["total_time"]], dtype=torch.float32, device=device)
            dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
            correct_sum = int(tensor[0].item())
            total_sum = int(tensor[1].item())
            total_time_sum = float(tensor[2].item())
            acc = (correct_sum / total_sum) if total_sum > 0 else 0.0
            avg_time = (total_time_sum / total_sum) if total_sum > 0 else 0.0
            agg_rows.append({
                "coverage": cov,
                "stride": st,
                "accuracy": acc,
                "avg_time": avg_time,
                "correct": correct_sum,
                "total": total_sum,
            })

        df_results = pd.DataFrame(agg_rows)
    else:
        df_results = evaluate_fixed_parallel(
            df=df,
            processor=processor,
            model=model,
            coverages=coverages,
            strides=strides,
            sample_size=sample_size,
            batch_size=batch_size,
            num_workers=workers,
            jitter_coverage_pct=jitter_coverage_pct,
        )

    # Save and log only rank 0
    if not ddp or rank == 0:
        # Ensure parent directory exists before saving
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        df_results.to_csv(out_path, index=False)

    # Log results to WandB (only rank 0 when DDP)
    if not args.no_wandb and (not ddp or rank == 0):
        wandb.log({"results_table": wandb.Table(dataframe=df_results)})
        
        # Log Pareto frontier
        frontier = compute_pareto(df_results)
        if not frontier.empty:
            wandb.log({"pareto_frontier": wandb.Table(dataframe=frontier)})
        
        # Log summary metrics
        best_acc = df_results["accuracy"].max()
        best_config = df_results.loc[df_results["accuracy"].idxmax()]
        wandb.summary["best_accuracy"] = best_acc
        wandb.summary["best_coverage"] = best_config["coverage"]
        wandb.summary["best_stride"] = best_config["stride"]

    if not ddp or rank == 0:
        if PLOTTING_AVAILABLE:
            plot_accuracy_curves(df_results)
            plot_heatmap(df_results)
        else:
            print("Plotting skipped (matplotlib/seaborn not installed)")

    # Optional per-class analysis (rank 0 only)
    if do_per_class and (not ddp or rank == 0):
        subset_size = len(df) if per_class_sample_size <= 0 else min(per_class_sample_size, len(df))
        df_per_class = per_class_analysis_fast(
            df=df,
            processor=processor,
            model=model,
            coverages=coverages,
            strides=strides,
            sample_size=subset_size,
            batch_size=batch_size,
            num_workers=workers,
        )
        os.makedirs(os.path.dirname(per_class_out), exist_ok=True)
        df_per_class.to_csv(per_class_out, index=False)

        if not args.no_wandb:
            wandb.log({"per_class_table": wandb.Table(dataframe=df_per_class)})

            # Summaries: best per class and top aliasing drop (100% vs 25% coverage mean across strides)
            best_per_class = (
                df_per_class.sort_values(["class", "accuracy"], ascending=[True, False])
                .groupby("class", as_index=False)
                .first()[["class", "coverage", "stride", "accuracy", "n_samples"]]
            )
            wandb.log({"per_class_best": wandb.Table(dataframe=best_per_class)})

            # Aliasing sensitivity: drop from 100% to 25%
            pivot_drop = (
                df_per_class[df_per_class["coverage"].isin([25, 100])]
                .groupby(["class", "coverage"], as_index=False)["accuracy"].mean()
                .pivot(index="class", columns="coverage", values="accuracy")
            )
            if 25 in pivot_drop.columns and 100 in pivot_drop.columns:
                drop_df = pivot_drop.reset_index().rename(columns={100: "acc_100", 25: "acc_25"})
                drop_df["drop_100_minus_25"] = drop_df["acc_100"] - drop_df["acc_25"]
                drop_df = drop_df.sort_values("drop_100_minus_25", ascending=False).head(20)
                wandb.log({"per_class_aliasing_drop": wandb.Table(dataframe=drop_df)})
    
    if not args.no_wandb and (not ddp or rank == 0):
        wandb.finish()

    # Cleanup DDP
    if ddp:
        dist.destroy_process_group()


if __name__ == "__main__":
    main()
