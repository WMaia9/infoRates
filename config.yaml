# Training Configuration for InfoRates UCF-101
# Updated: December 20, 2025 - Multi-Model Support

# ===== MODEL SELECTION =====
# Choose model: timesformer | videomae | vivit
model_name: "timesformer"

# ===== TRAINING HYPERPARAMETERS =====
train_epochs: 5
train_batch_size: 15                         # Batch size per GPU
train_learning_rate: 1e-5
train_gradient_accumulation_steps: 1          # Use >1 for memory efficiency
train_cleanup_interval: 0                     # Clean memory every N steps (0 = only at epoch boundaries)
train_video_root: "data/UCF101_data/UCF-101"
train_save_path: "fine_tuned_models"
train_num_workers: 4
train_pin_memory: true
train_use_ddp: false                          # Set to true for multi-GPU
train_num_gpus: 1
train_device: "cuda"

# ===== W&B LOGGING =====
train_wandb_project: "inforates-ucf101"
train_wandb_run_name: "fine-tuning-multimodel"
train_disable_wandb: false

# ===== EVALUATION SETTINGS =====
eval_results_dir: "data/UCF101_data/results"
eval_batch_size: 38
eval_sample_size: 0                           # 0 = full dataset, N = limit to N samples
eval_device: "cuda"
eval_wandb: false                             # Optional W&B logging

# ===== TEMPORAL SAMPLING =====
# Configurations to test: coverage (%) Ã— stride
eval_coverages: [10, 25, 50, 75, 100]
eval_strides: [1, 2, 4, 8, 16]

# ===== MODEL-SPECIFIC DEFAULTS =====
# Frame counts per model (auto-handled by scripts, but documented here):
# - TimeSformer:  8 frames (divided space-time attention)
# - VideoMAE:    16 frames (masked autoencoder pretraining)
# - ViViT:       32 frames (pure vision transformer)
#
# Models are automatically loaded from Hugging Face:
# - TimeSformer: facebook/timesformer-base-finetuned-k400
# - VideoMAE:    MCG-NJU/videomae-base-finetuned-kinetics
# - ViViT:       google/vivit-b-16x2

